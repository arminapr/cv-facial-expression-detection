{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ax_fWPSrb1fF",
   "metadata": {
    "id": "ax_fWPSrb1fF"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple convolutional network with the following architecture:\n",
    "\n",
    "    [conv - bn - relu] x M - global_average_pooling - affine - softmax\n",
    "\n",
    "    \"[conv - bn - relu] x M\" means the \"conv-bn-relu\" block is repeated for\n",
    "    M times, where M is implicitly defined by the convolution layers' parameters.\n",
    "    Whether to use the batch normalization layer (bn) in-between is a design choice.\n",
    "\n",
    "    For each convolution layer, we do downsampling of factor 2 by setting the stride\n",
    "    to be 2. So we can have a large receptive field size.\n",
    "\n",
    "    The network operates on minibatches of data that have shape (N, C, H, W)\n",
    "    consisting of N images, each with height H and width W and with C input\n",
    "    channels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim=(3, 32, 32), filter_sizes=[7], filter_channels=[32],\n",
    "            num_classes=10, use_batch_norm=True):\n",
    "        \"\"\"\n",
    "        Initialize a new CNN.\n",
    "\n",
    "        Inputs:\n",
    "        - input_dim: Tuple (C, H, W) giving size of input data\n",
    "        - filter_sizes: Width/height of filters to use in the convolutional layer. It is a\n",
    "          list whose length defines the number of convolution layers.\n",
    "        - filter_channels: Number of filters to use in each convolutional layer. It has the\n",
    "          same length as filter_sizes.\n",
    "        - num_classes: Number of output classes\n",
    "        - use_batch_norm: A boolean variable indicating whether to use batch normalization\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        assert len(filter_sizes) == len(filter_channels), \"Inconsistent filter sizes and channels.\"\n",
    "\n",
    "        ############################################################################\n",
    "        # TODO: Define a set of layers according to the user input.                #\n",
    "        #                                                                          #\n",
    "        # IMPORTANT:                                                               #\n",
    "        # 1. For this assignment, you can assume that the padding of the every     #\n",
    "        # convolutional layer are chosen so that **the width and height of the     #\n",
    "        # input are preserved** (without considering the stride). You need to      #\n",
    "        # carefully set the `pad` parameter for the convolution.                   #\n",
    "        #                                                                          #\n",
    "        # 2. For each convolution layer, we use stride of 2 to do downsampling.    #\n",
    "        ############################################################################\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        C, H, W = input_dim\n",
    "        layers = []\n",
    "        in_channels = C\n",
    "\n",
    "        # build each conv-bn-relu block\n",
    "        for i in range(len(filter_sizes)):\n",
    "            filter_size = filter_sizes[i]\n",
    "            out_channels = filter_channels[i]\n",
    "\n",
    "            # padding to preserve H and W\n",
    "            padding = (filter_size - 1) // 2\n",
    "            stride = 2\n",
    "            use_bias = not use_batch_norm\n",
    "\n",
    "            # define the conv layer\n",
    "            curr_layer = nn.Conv2d(in_channels, out_channels, kernel_size= filter_size,\n",
    "                                    stride= stride, padding=padding, bias= use_bias)\n",
    "\n",
    "            layers.append(curr_layer)\n",
    "            if use_batch_norm:\n",
    "                layers.append(nn.BatchNorm2d(out_channels))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "\n",
    "            in_channels = out_channels\n",
    "        # stack the layers together in order\n",
    "        self.conv_layers = nn.Sequential(*layers)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(in_channels, num_classes)\n",
    "        ############################################################################\n",
    "        #                             END OF YOUR CODE                             #\n",
    "        ############################################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = None\n",
    "        feat_before_gap = None\n",
    "\n",
    "        ############################################################################\n",
    "        # TODO: Implement the forward pass for the simple convolutional net,       #\n",
    "        # computing the class scores for x and storing them in the logits          #\n",
    "        # variable. Also, store the feature map right before the global average    #\n",
    "        # pooling (GAP) layer in the feat_before_gap variable for debugging        #\n",
    "        # purpose only.                                                            #\n",
    "        ############################################################################\n",
    "        out = self.conv_layers(x)\n",
    "        feat_before_gap = out\n",
    "        out = self.global_avg_pool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        # we won't apply softmax directly because we are gonna use cross entropy loss\n",
    "        logits = self.fc(out)\n",
    "        ############################################################################\n",
    "        #                             END OF YOUR CODE                             #\n",
    "        ############################################################################\n",
    "\n",
    "        return logits, feat_before_gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1bed3155",
   "metadata": {
    "id": "1bed3155"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Function to test an already trained model\n",
    "def test_model(model, data_loader):\n",
    "    \"\"\"\n",
    "    Compute accuracy of the model.\n",
    "\n",
    "    Inputs:\n",
    "      - model: A CNN implemented in PyTorch\n",
    "      - data_loader: A data loader that will provide batched images and labels\n",
    "    \"\"\"\n",
    "\n",
    "    # set the model in evaluation mode so the batch norm layers will behave correctly\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for batch_data in data_loader:\n",
    "            images, labels = batch_data\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "            labels = labels.long()\n",
    "\n",
    "            predicted = None\n",
    "            ############################################################################\n",
    "            # TODO: Compute the predicted labels of the batched input images and store #\n",
    "            # them in the predicted varaible.                                          #\n",
    "            ############################################################################\n",
    "            outputs, _ = model(images)\n",
    "            # torch max returns the max value and the index at whih they happen at\n",
    "            # we use the index as the predicted label\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            ############################################################################\n",
    "            #                             END OF YOUR CODE                             #\n",
    "            ############################################################################\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = 100 * correct // total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "53d70097",
   "metadata": {
    "id": "53d70097"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_val_model(model, train_data_loader, val_data_loader, loss_fn, optimizer, lr_scheduler, num_epochs, print_freq=50):\n",
    "\n",
    "    \"\"\"\n",
    "    Training and validating a CNN model using PyTorch.\n",
    "\n",
    "    Inputs:\n",
    "      - model: A CNN implemented in PyTorch\n",
    "      - data_loader: A data loader that will provide batched images and labels\n",
    "      - loss_fn: A loss function (e.g., cross entropy loss)\n",
    "      - lr_scheduler: Learning rate scheduler\n",
    "      - num_epochs: Number of epochs in total\n",
    "      - print_freq: Frequency to print training statistics\n",
    "\n",
    "    Output:\n",
    "      - model: Trained CNN model\n",
    "    \"\"\"\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        # set the model in the train mode so the batch norm layers will behave correctly\n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_total = 0.0\n",
    "        running_correct = 0.0\n",
    "        for i, batch_data in enumerate(train_data_loader):\n",
    "            # Every data instance is an image + label pair\n",
    "            images, labels = batch_data\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "            labels = labels.long()\n",
    "\n",
    "            predicted = None\n",
    "            ############################################################################\n",
    "            # TODO: Finish loss computation, gradient backpropagation, weight update,  #\n",
    "            # and computing the predicted labels of the input images and store them in #\n",
    "            # the predicted varaible, which will be used to monitor the training       #\n",
    "            # accuracy.                                                                #\n",
    "            #                                                                          #\n",
    "            # Note: The learning rate is updated after each **epoch**.                 #\n",
    "            ############################################################################\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            ############################################################################\n",
    "            #                             END OF YOUR CODE                             #\n",
    "            ############################################################################\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            running_total += labels.size(0)\n",
    "            running_correct += (predicted == labels).sum().item()\n",
    "            if i % print_freq == 0:    # print every certain number of mini-batches\n",
    "                running_loss = running_loss / print_freq\n",
    "                running_acc = running_correct / running_total * 100\n",
    "                last_lr = lr_scheduler.get_last_lr()[0]\n",
    "                print(f'[{epoch_i + 1}/{num_epochs}, {i + 1:5d}/{len(train_data_loader)}] loss: {running_loss:.3f} acc: {running_acc:.3f} lr: {last_lr:.5f}')\n",
    "                running_loss = 0.0\n",
    "                running_total = 0.0\n",
    "                running_correct = 0.0\n",
    "\n",
    "        # adjust the learning rate\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        val_acc = test_model(model, val_data_loader)\n",
    "        print(f'[{epoch_i + 1}/{num_epochs}] val acc: {val_acc:.3f}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a4629882",
   "metadata": {
    "id": "a4629882"
   },
   "outputs": [],
   "source": [
    "def set_up_loss_optimizer_lr_scheduler(model, learning_rate, momentum, lr_step_size, lr_gamma):\n",
    "    \"\"\"\n",
    "    In this programming assignment, we will adopt the most common choice for the optimizer:\n",
    "    SGD + momentum and learning rate scheduler: StepLR. Please refer to https://pytorch.org/docs/stable/optim.html#algorithms\n",
    "    and https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR for more details.\n",
    "    \"\"\"\n",
    "    loss_fn = None\n",
    "    optimizer = None\n",
    "    lr_scheduler = None\n",
    "\n",
    "    ############################################################################\n",
    "    # TODO: Define the loss function, optimizer (SGD + momentum), and          #\n",
    "    # learning rate scheduler (StepLR).                                        #\n",
    "    #                                                                          #\n",
    "    # Note: We expect you to set up the learning rate in an epoch-based way.   #\n",
    "    # We will run the learning rate scheduler after each epoch.                #\n",
    "    ############################################################################\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr= learning_rate, momentum= momentum)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size= lr_step_size, gamma= lr_gamma)\n",
    "    ############################################################################\n",
    "    #                             END OF YOUR CODE                             #\n",
    "    ############################################################################\n",
    "\n",
    "    return loss_fn, optimizer, lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3450a407-b4ae-4d76-87ca-ad5fa25eb987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch\n",
    "\n",
    "def get_dataloaders(data_dir=\"../datasets/fer2013\", batch_size=64, val_split=0.1, \n",
    "                   model_type='efficient', augmentation=True):\n",
    "    \"\"\"\n",
    "    Get data loaders for FER2013 dataset.\n",
    "    \n",
    "    Args:\n",
    "        model_type: 'efficient' for our CNN, 'resnet' for ResNet\n",
    "        augmentation: Whether to use data augmentation for training\n",
    "    \"\"\"\n",
    "    print(\"hi\", data_dir)\n",
    "    if data_dir is None:\n",
    "        data_dir = os.path.join(os.path.dirname(__file__), \"..\", \"datasets\", \"fer2013\")\n",
    "        data_dir = os.path.abspath(data_dir)\n",
    "    \n",
    "    if model_type == 'resnet':\n",
    "        # original ResNet configuration\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=3), # ResNet expects 3 channels\n",
    "            transforms.Resize((224, 224)), # Resnet input size is (224, 224)\n",
    "            transforms.ToTensor(),\n",
    "            # Mean and std come from ImageNet dataset used to train ResNet\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        transform_test = transform_train\n",
    "        \n",
    "    else: # our CNN model\n",
    "        if augmentation:\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.Grayscale(num_output_channels=1),\n",
    "                transforms.Resize((48, 48)),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomRotation(degrees=10),\n",
    "                transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "                transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.5], std=[0.5]),  # Grayscale normalization\n",
    "                transforms.RandomErasing(p=0.1, scale=(0.02, 0.33))  # Cutout augmentation\n",
    "            ])\n",
    "        else:\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.Grayscale(num_output_channels=1),\n",
    "                transforms.Resize((48, 48)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "            ])\n",
    "        \n",
    "        # Test/val without augmentation\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.Resize((48, 48)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "        ])\n",
    "    \n",
    "    # Load datasets\n",
    "    training_data = datasets.ImageFolder(os.path.join(data_dir, \"train\"), transform=transform_train)\n",
    "    testing_data = datasets.ImageFolder(os.path.join(data_dir, \"test\"), transform=transform_test)\n",
    "    # training data validation\n",
    "    validation_size = int(len(training_data) * val_split)\n",
    "    training_size = len(training_data) - validation_size\n",
    "    \n",
    "    # Use a fixed seed for reproducible splits\n",
    "    generator = torch.Generator().manual_seed(42)\n",
    "    training_data, validation_data = random_split(\n",
    "        training_data, \n",
    "        [training_size, validation_size],\n",
    "        generator=generator\n",
    "    )\n",
    "    \n",
    "    # For validation data, we need to override the transform to remove augmentation\n",
    "    if model_type != 'resnet':\n",
    "        validation_data.dataset.transform = transform_test\n",
    "    \n",
    "    # Create data loaders with optimized settings\n",
    "    training_loader = DataLoader(\n",
    "        training_data, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=4,  # Increase if you have more CPU cores\n",
    "        pin_memory=True,  # Faster GPU transfer\n",
    "        persistent_workers=True  # Keep workers alive between epochs\n",
    "    )\n",
    "    \n",
    "    validation_loader = DataLoader(\n",
    "        validation_data, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    \n",
    "    testing_loader = DataLoader(\n",
    "        testing_data, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    \n",
    "    num_classes = len(testing_data.classes)\n",
    "    print(f\"Dataset loaded: {training_size} train, {validation_size} val, {len(testing_data)} test samples\")\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    \n",
    "    return training_loader, validation_loader, testing_loader, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1db20265",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "id": "1db20265",
    "outputId": "4faf4139-9fb6-4422-e64e-d9ba06ef29a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv layer 0: in_channels=1, out_channels=64\n",
      "Conv layer 3: in_channels=64, out_channels=128\n",
      "Conv layer 6: in_channels=128, out_channels=256\n",
      "Conv layer 9: in_channels=256, out_channels=512\n",
      "Number of parameters: 1555.914K\n",
      "hi ../datasets/fer2013\n",
      "Dataset loaded: 25839 train, 2870 val, 7178 test samples\n",
      "Number of classes: 7\n",
      "Batch size: 16\n",
      "[1/3,     1/1615] loss: 0.048 acc: 0.000 lr: 0.00100\n",
      "[1/3,    51/1615] loss: 1.981 acc: 22.625 lr: 0.00100\n",
      "[1/3,   101/1615] loss: 1.993 acc: 21.125 lr: 0.00100\n",
      "[1/3,   151/1615] loss: 1.971 acc: 22.750 lr: 0.00100\n",
      "[1/3,   201/1615] loss: 1.959 acc: 22.500 lr: 0.00100\n",
      "[1/3,   251/1615] loss: 1.865 acc: 23.500 lr: 0.00100\n",
      "[1/3,   301/1615] loss: 1.776 acc: 28.250 lr: 0.00100\n",
      "[1/3,   351/1615] loss: 1.815 acc: 27.125 lr: 0.00100\n",
      "[1/3,   401/1615] loss: 1.781 acc: 26.875 lr: 0.00100\n",
      "[1/3,   451/1615] loss: 1.828 acc: 28.000 lr: 0.00100\n",
      "[1/3,   501/1615] loss: 1.771 acc: 27.875 lr: 0.00100\n",
      "[1/3,   551/1615] loss: 1.835 acc: 25.750 lr: 0.00100\n",
      "[1/3,   601/1615] loss: 1.736 acc: 32.250 lr: 0.00100\n",
      "[1/3,   651/1615] loss: 1.705 acc: 31.000 lr: 0.00100\n",
      "[1/3,   701/1615] loss: 1.712 acc: 32.125 lr: 0.00100\n",
      "[1/3,   751/1615] loss: 1.712 acc: 34.875 lr: 0.00100\n",
      "[1/3,   801/1615] loss: 1.710 acc: 34.000 lr: 0.00100\n",
      "[1/3,   851/1615] loss: 1.747 acc: 33.000 lr: 0.00100\n",
      "[1/3,   901/1615] loss: 1.732 acc: 31.875 lr: 0.00100\n",
      "[1/3,   951/1615] loss: 1.683 acc: 36.500 lr: 0.00100\n",
      "[1/3,  1001/1615] loss: 1.691 acc: 33.375 lr: 0.00100\n",
      "[1/3,  1051/1615] loss: 1.607 acc: 35.875 lr: 0.00100\n",
      "[1/3,  1101/1615] loss: 1.575 acc: 36.875 lr: 0.00100\n",
      "[1/3,  1151/1615] loss: 1.637 acc: 33.125 lr: 0.00100\n",
      "[1/3,  1201/1615] loss: 1.600 acc: 36.125 lr: 0.00100\n",
      "[1/3,  1251/1615] loss: 1.674 acc: 36.875 lr: 0.00100\n",
      "[1/3,  1301/1615] loss: 1.585 acc: 37.625 lr: 0.00100\n",
      "[1/3,  1351/1615] loss: 1.619 acc: 36.375 lr: 0.00100\n",
      "[1/3,  1401/1615] loss: 1.583 acc: 39.250 lr: 0.00100\n",
      "[1/3,  1451/1615] loss: 1.592 acc: 38.375 lr: 0.00100\n",
      "[1/3,  1501/1615] loss: 1.568 acc: 39.250 lr: 0.00100\n",
      "[1/3,  1551/1615] loss: 1.591 acc: 38.500 lr: 0.00100\n",
      "[1/3,  1601/1615] loss: 1.582 acc: 39.000 lr: 0.00100\n",
      "[1/3] val acc: 38.000\n",
      "[2/3,     1/1615] loss: 0.028 acc: 56.250 lr: 0.00100\n",
      "[2/3,    51/1615] loss: 1.565 acc: 39.750 lr: 0.00100\n",
      "[2/3,   101/1615] loss: 1.623 acc: 38.500 lr: 0.00100\n",
      "[2/3,   151/1615] loss: 1.505 acc: 39.250 lr: 0.00100\n",
      "[2/3,   201/1615] loss: 1.496 acc: 39.500 lr: 0.00100\n",
      "[2/3,   251/1615] loss: 1.541 acc: 39.625 lr: 0.00100\n",
      "[2/3,   301/1615] loss: 1.535 acc: 41.125 lr: 0.00100\n",
      "[2/3,   351/1615] loss: 1.507 acc: 37.500 lr: 0.00100\n",
      "[2/3,   401/1615] loss: 1.592 acc: 38.875 lr: 0.00100\n",
      "[2/3,   451/1615] loss: 1.554 acc: 39.375 lr: 0.00100\n",
      "[2/3,   501/1615] loss: 1.558 acc: 38.125 lr: 0.00100\n",
      "[2/3,   551/1615] loss: 1.538 acc: 40.375 lr: 0.00100\n",
      "[2/3,   601/1615] loss: 1.489 acc: 43.375 lr: 0.00100\n",
      "[2/3,   651/1615] loss: 1.503 acc: 42.000 lr: 0.00100\n",
      "[2/3,   701/1615] loss: 1.475 acc: 41.375 lr: 0.00100\n",
      "[2/3,   751/1615] loss: 1.528 acc: 41.750 lr: 0.00100\n",
      "[2/3,   801/1615] loss: 1.573 acc: 37.875 lr: 0.00100\n",
      "[2/3,   851/1615] loss: 1.524 acc: 40.625 lr: 0.00100\n",
      "[2/3,   901/1615] loss: 1.547 acc: 39.750 lr: 0.00100\n",
      "[2/3,   951/1615] loss: 1.463 acc: 43.625 lr: 0.00100\n",
      "[2/3,  1001/1615] loss: 1.544 acc: 40.000 lr: 0.00100\n",
      "[2/3,  1051/1615] loss: 1.450 acc: 42.000 lr: 0.00100\n",
      "[2/3,  1101/1615] loss: 1.456 acc: 43.750 lr: 0.00100\n",
      "[2/3,  1151/1615] loss: 1.494 acc: 42.750 lr: 0.00100\n",
      "[2/3,  1201/1615] loss: 1.487 acc: 39.125 lr: 0.00100\n",
      "[2/3,  1251/1615] loss: 1.516 acc: 42.375 lr: 0.00100\n",
      "[2/3,  1301/1615] loss: 1.438 acc: 43.250 lr: 0.00100\n",
      "[2/3,  1351/1615] loss: 1.440 acc: 46.250 lr: 0.00100\n",
      "[2/3,  1401/1615] loss: 1.403 acc: 44.500 lr: 0.00100\n",
      "[2/3,  1451/1615] loss: 1.454 acc: 42.375 lr: 0.00100\n",
      "[2/3,  1501/1615] loss: 1.430 acc: 45.375 lr: 0.00100\n",
      "[2/3,  1551/1615] loss: 1.417 acc: 46.500 lr: 0.00100\n",
      "[2/3,  1601/1615] loss: 1.439 acc: 44.000 lr: 0.00100\n",
      "[2/3] val acc: 44.000\n",
      "[3/3,     1/1615] loss: 0.033 acc: 31.250 lr: 0.00100\n",
      "[3/3,    51/1615] loss: 1.322 acc: 51.125 lr: 0.00100\n",
      "[3/3,   101/1615] loss: 1.308 acc: 50.875 lr: 0.00100\n",
      "[3/3,   151/1615] loss: 1.339 acc: 50.625 lr: 0.00100\n",
      "[3/3,   201/1615] loss: 1.395 acc: 45.500 lr: 0.00100\n",
      "[3/3,   251/1615] loss: 1.382 acc: 48.375 lr: 0.00100\n",
      "[3/3,   301/1615] loss: 1.338 acc: 47.625 lr: 0.00100\n",
      "[3/3,   351/1615] loss: 1.359 acc: 47.625 lr: 0.00100\n",
      "[3/3,   401/1615] loss: 1.335 acc: 50.250 lr: 0.00100\n",
      "[3/3,   451/1615] loss: 1.302 acc: 51.000 lr: 0.00100\n",
      "[3/3,   501/1615] loss: 1.390 acc: 47.875 lr: 0.00100\n",
      "[3/3,   551/1615] loss: 1.378 acc: 47.000 lr: 0.00100\n",
      "[3/3,   601/1615] loss: 1.329 acc: 49.000 lr: 0.00100\n",
      "[3/3,   651/1615] loss: 1.387 acc: 47.750 lr: 0.00100\n",
      "[3/3,   701/1615] loss: 1.307 acc: 50.875 lr: 0.00100\n",
      "[3/3,   751/1615] loss: 1.352 acc: 48.250 lr: 0.00100\n",
      "[3/3,   801/1615] loss: 1.387 acc: 47.375 lr: 0.00100\n",
      "[3/3,   851/1615] loss: 1.397 acc: 44.750 lr: 0.00100\n",
      "[3/3,   901/1615] loss: 1.328 acc: 48.625 lr: 0.00100\n",
      "[3/3,   951/1615] loss: 1.366 acc: 47.250 lr: 0.00100\n",
      "[3/3,  1001/1615] loss: 1.392 acc: 45.750 lr: 0.00100\n",
      "[3/3,  1051/1615] loss: 1.292 acc: 49.250 lr: 0.00100\n",
      "[3/3,  1101/1615] loss: 1.348 acc: 47.750 lr: 0.00100\n",
      "[3/3,  1151/1615] loss: 1.401 acc: 47.250 lr: 0.00100\n",
      "[3/3,  1201/1615] loss: 1.404 acc: 44.375 lr: 0.00100\n",
      "[3/3,  1251/1615] loss: 1.375 acc: 47.500 lr: 0.00100\n",
      "[3/3,  1301/1615] loss: 1.299 acc: 51.500 lr: 0.00100\n",
      "[3/3,  1351/1615] loss: 1.310 acc: 48.250 lr: 0.00100\n",
      "[3/3,  1401/1615] loss: 1.375 acc: 50.000 lr: 0.00100\n",
      "[3/3,  1451/1615] loss: 1.283 acc: 51.875 lr: 0.00100\n",
      "[3/3,  1501/1615] loss: 1.388 acc: 47.750 lr: 0.00100\n",
      "[3/3,  1551/1615] loss: 1.342 acc: 48.250 lr: 0.00100\n",
      "[3/3,  1601/1615] loss: 1.329 acc: 48.875 lr: 0.00100\n",
      "[3/3] val acc: 47.000\n",
      "testing accuracy: 48.000\n"
     ]
    }
   ],
   "source": [
    "# In practice, this is a hyperparameter to tune.\n",
    "# But here we use a fixed number to make the comparisons fair.\n",
    "num_epochs = 3\n",
    "\n",
    "model = None\n",
    "loss_fn = None\n",
    "optimizer = None\n",
    "lr_scheduler = None\n",
    "############################################################################\n",
    "# TODO: Set up and tune the hyper parameters.                              #\n",
    "############################################################################\n",
    "batch_size = 16\n",
    "learning_rate = 0.001\n",
    "momentum = 0.99\n",
    "lr_gamma = 1\n",
    "\n",
    "model = ConvNet(input_dim=(1, 48, 48),\n",
    "                filter_sizes=[3, 3, 3, 3],\n",
    "                filter_channels=[64, 128, 256, 512], \n",
    "                use_batch_norm=True)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr= learning_rate, momentum= momentum)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size= 1, gamma= lr_gamma)\n",
    "############################################################################\n",
    "#                             END OF YOUR CODE                             #\n",
    "############################################################################\n",
    "\n",
    "model = model.cuda()\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print('Number of parameters: {:.3f}K'.format(num_params / 1000))\n",
    "\n",
    "# set up the data loaders\n",
    "# note the usage of the batch_size hyperparameter here\n",
    "train_loader, val_loader, test_loader, _ = get_dataloaders(model_type='cnn', batch_size=batch_size)\n",
    "\n",
    "model = train_val_model(model, train_loader, val_loader, loss_fn, optimizer, lr_scheduler, num_epochs)\n",
    "test_acc = test_model(model, test_loader)\n",
    "print(f\"testing accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0901dc49",
   "metadata": {
    "id": "0901dc49"
   },
   "outputs": [],
   "source": [
    "# Resnet like CNN\n",
    "from functools import partial\n",
    "from typing import Any, Callable, List, Optional, Type, Union\n",
    "from torchvision.models.resnet import conv1x1, conv3x3, BasicBlock, Bottleneck, ResNet\n",
    "from torch import Tensor\n",
    "\n",
    "class MyResNet(ResNet):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        layers: List[int],\n",
    "        num_classes: int = 1000,\n",
    "        zero_init_residual: bool = False,\n",
    "        groups: int = 1,\n",
    "        width_per_group: int = 64,\n",
    "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Here we will design a model architecture MyResNet, inherited from the ResNet model.\n",
    "        First check here https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py about the\n",
    "        implementation of ResNet in PyTorch.\n",
    "        What you need to do in this part is the remove the layer3 and layer4 and also modify the final\n",
    "        fully-connected layer accordingly.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            block, layers, num_classes, zero_init_residual, groups,\n",
    "            width_per_group, replace_stride_with_dilation, norm_layer\n",
    "        )\n",
    "\n",
    "        ############################################################################\n",
    "        # TODO: Remove the layer3 and layer4 block in the original implementation  #\n",
    "        # of ResNet and modify the fully-connected layer (classifier) accordingly. #\n",
    "        ############################################################################\n",
    "        self.layer3 = nn.Identity()\n",
    "        self.layer4 = nn.Identity()\n",
    "        self.fc = nn.Linear(128 * block.expansion, num_classes)\n",
    "        ############################################################################\n",
    "        #                             END OF YOUR CODE                             #\n",
    "        ############################################################################\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        logits = None\n",
    "        feat_before_gap = None\n",
    "        ############################################################################\n",
    "        # TODO: Implement the forward pass for the ResNet-like model,              #\n",
    "        # computing the class scores for x and storing them in the logits          #\n",
    "        # variable. Also, store the feature map right before the global average    #\n",
    "        # pooling (GAP) layer in the feat_before_gap variable for debugging        #\n",
    "        # purpose only.                                                            #\n",
    "        ############################################################################\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        # pass through layer1 and layer2 only\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        feat_before_gap = x\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        logits = self.fc(x)\n",
    "        ############################################################################\n",
    "        #                             END OF YOUR CODE                             #\n",
    "        ############################################################################\n",
    "\n",
    "        return logits, feat_before_gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8bc4b6de",
   "metadata": {
    "id": "8bc4b6de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 684.362K\n",
      "hi ../datasets/fer2013\n",
      "Dataset loaded: 25839 train, 2870 val, 7178 test samples\n",
      "Number of classes: 7\n",
      "Batch size: 64\n",
      "[1/3,     1/404] loss: 0.054 acc: 3.125 lr: 0.00100\n",
      "[1/3,    51/404] loss: 2.029 acc: 19.438 lr: 0.00100\n",
      "[1/3,   101/404] loss: 1.910 acc: 22.625 lr: 0.00100\n",
      "[1/3,   151/404] loss: 1.848 acc: 24.281 lr: 0.00100\n",
      "[1/3,   201/404] loss: 1.836 acc: 24.562 lr: 0.00100\n",
      "[1/3,   251/404] loss: 1.811 acc: 24.875 lr: 0.00100\n",
      "[1/3,   301/404] loss: 1.821 acc: 23.906 lr: 0.00100\n",
      "[1/3,   351/404] loss: 1.788 acc: 25.156 lr: 0.00100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m train_loader, val_loader, test_loader, _ \u001b[38;5;241m=\u001b[39m get_dataloaders(model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresnet\u001b[39m\u001b[38;5;124m'\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[1;32m     28\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m---> 29\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_val_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m test_acc \u001b[38;5;241m=\u001b[39m test_model(model, test_loader)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtesting accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[35], line 53\u001b[0m, in \u001b[0;36mtrain_val_model\u001b[0;34m(model, train_data_loader, val_data_loader, loss_fn, optimizer, lr_scheduler, num_epochs, print_freq)\u001b[0m\n\u001b[1;32m     47\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m############################################################################\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m#                             END OF YOUR CODE                             #\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m############################################################################\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# print statistics\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m running_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     55\u001b[0m running_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (predicted \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# In practice, this is a hyperparameter to tune.\n",
    "# But here we use a fixed number to make the comparisons fair.\n",
    "num_epochs = 3\n",
    "\n",
    "model = MyResNet(BasicBlock, [2, 2, 2, 2], num_classes=10)\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print('Number of parameters: {:.3f}K'.format(num_params / 1000))\n",
    "\n",
    "############################################################################\n",
    "# TODO: Set up and tune the hyper parameters.                              #\n",
    "############################################################################\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "momentum = 0.99\n",
    "lr_gamma = 0.99\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr= learning_rate, momentum= momentum)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size= 1, gamma= lr_gamma)\n",
    "############################################################################\n",
    "#                             END OF YOUR CODE                             #\n",
    "############################################################################\n",
    "\n",
    "# set up the data loaders\n",
    "# note the usage of the batch_size hyperparameter here\n",
    "train_loader, val_loader, test_loader, _ = get_dataloaders(model_type='resnet', batch_size=batch_size)\n",
    "\n",
    "model = model.cuda()\n",
    "model = train_val_model(model, train_loader, val_loader, loss_fn, optimizer, lr_scheduler, num_epochs)\n",
    "test_acc = test_model(model, test_loader)\n",
    "print(f\"testing accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2b772be2",
   "metadata": {
    "id": "2b772be2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/eai/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /home/parvareshrizi.a/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [00:01<00:00, 73.0MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 684.362K\n",
      "hi ../datasets/fer2013\n",
      "Dataset loaded: 25839 train, 2870 val, 7178 test samples\n",
      "Number of classes: 7\n",
      "Batch size: 128\n",
      "[1/3,     1/202] loss: 0.046 acc: 3.906 lr: 0.00100\n",
      "[1/3,    51/202] loss: 2.235 acc: 15.656 lr: 0.00100\n",
      "[1/3,   101/202] loss: 2.096 acc: 25.297 lr: 0.00100\n",
      "[1/3,   151/202] loss: 2.014 acc: 24.500 lr: 0.00100\n",
      "[1/3,   201/202] loss: 1.961 acc: 25.031 lr: 0.00100\n",
      "[1/3] val acc: 25.000\n",
      "[2/3,     1/202] loss: 0.040 acc: 21.094 lr: 0.00095\n",
      "[2/3,    51/202] loss: 1.922 acc: 25.766 lr: 0.00095\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m train_loader, val_loader, test_loader, _ \u001b[38;5;241m=\u001b[39m get_dataloaders(model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresnet\u001b[39m\u001b[38;5;124m'\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[1;32m     43\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m---> 44\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_val_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m test_acc \u001b[38;5;241m=\u001b[39m test_model(model, test_loader)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtesting accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[35], line 53\u001b[0m, in \u001b[0;36mtrain_val_model\u001b[0;34m(model, train_data_loader, val_data_loader, loss_fn, optimizer, lr_scheduler, num_epochs, print_freq)\u001b[0m\n\u001b[1;32m     47\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m############################################################################\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m#                             END OF YOUR CODE                             #\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m############################################################################\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# print statistics\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m running_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     55\u001b[0m running_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (predicted \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Let's experiment with transfer learning by borrowing the weights of a ResNet model pre-trained on ImageNet.\n",
    "import torchvision\n",
    "imagenet_resnet50 = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights)\n",
    "model = MyResNet(BasicBlock, [2, 2, 2, 2], num_classes=10)\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print('Number of parameters: {:.3f}K'.format(num_params / 1000))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "imagenet_resnet18 = imagenet_resnet18.to(device)\n",
    "\n",
    "############################################################################\n",
    "# TODO: Copy the appropriate weights from imagenet_resnet18 to our custom  #\n",
    "# model, which shares part of the network architecture.                    #\n",
    "############################################################################\n",
    "model.conv1.weight.data = imagenet_resnet18.conv1.weight.data\n",
    "model.bn1.weight.data = imagenet_resnet18.bn1.weight.data\n",
    "model.bn1.bias.data = imagenet_resnet18.bn1.bias.data\n",
    "model.layer1.load_state_dict(imagenet_resnet18.layer1.state_dict())\n",
    "model.layer2.load_state_dict(imagenet_resnet18.layer2.state_dict())\n",
    "############################################################################\n",
    "#                             END OF YOUR CODE                             #\n",
    "############################################################################\n",
    "\n",
    "############################################################################\n",
    "# TODO: Set up and tune the hyper parameters.                              #\n",
    "############################################################################\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "lr_gamma = 0.95\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr= learning_rate, momentum= momentum)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size= 1, gamma= lr_gamma)\n",
    "############################################################################\n",
    "#                             END OF YOUR CODE                             #\n",
    "############################################################################\n",
    "\n",
    "# set up the data loaders\n",
    "# note the usage of the batch_size hyperparameter here\n",
    "train_loader, val_loader, test_loader, _ = get_dataloaders(model_type='resnet', batch_size=batch_size)\n",
    "\n",
    "model = model.cuda()\n",
    "model = train_val_model(model, train_loader, val_loader, loss_fn, optimizer, lr_scheduler, num_epochs)\n",
    "test_acc = test_model(model, test_loader)\n",
    "print(f\"testing accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e0d8319-2c83-46f7-abe2-bcb23864e4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "MPS available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"MPS available:\", torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91836148-3c7d-4515-bcd5-112c066fffa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
